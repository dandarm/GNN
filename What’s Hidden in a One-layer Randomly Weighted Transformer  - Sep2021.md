[2109.03939.pdf]
[[randomnetwork]] [[ReservoirComputing]]
[[lotteryticket]]



We demonstrate that, hidden within one-layer randomly weighted neural networks, there exist subnetworks that can achieve impressive performance, without ever modifying the weight initializations, on machine translation tasks.

Using a fixed pretrained embedding layer, the previously found subnetworks are smaller than, but can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained Transformer
subnetworks of a randomly weighted neural network (NN) can achieve competitive performance


In this work, we first validate that there exist subnetworks of standard randomly weighted Transformers (Reservoir Transformers in (Shen et al., 2021) that can perform competitively with fully-weighted alternatives on machine translation and natural language understanding tasks



Conclusions:
In this paper, we validate the existence of effective subnetworks in a one-layer randomly weighted Transformer on translation tasks. Hidden within a one-layer randomly weighted Transformerwide/wider with fixed pre-trained embedding layers, we find there exist subnetworks that are smaller than, but can competitively match, the performance of a trained Transformersmall/base on IWSLT14/WMT14