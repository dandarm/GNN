[2012.15045.pdf]

[[ReservoirComputing]]
[[NLP]]


We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated







This work demonstrated that state-of-the-art transformer architectures can be trained without updating all of the layers.
We show that such reservoir transformers show better convergence rates and test-set generalization. We demonstrated that the backward pass can be skipped altogether, opening up exciting vanues for future research